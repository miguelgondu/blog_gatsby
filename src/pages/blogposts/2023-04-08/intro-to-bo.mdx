---
date: "2023-04-08"
title: "Bayesian Optimization using Gaussian Processes: an introduction"
summary: "A tutorial on Bayesian Optimization using GPyTorch and BoTorch, including an introduction to Gaussian Process Regression."
lang: "en"
show: true
imgsrc: "/assets/bo_blogpost/xsinx_gp_all_distributions.gif"
categories:
    - Bayesian Optimization
    - PyTorch
    - GPyTorch
    - BoTorch
    - Optimization
---

import CenteredImage from './CenteredImage'

# Bayesian Optimization using Gaussian Processes: an introduction

> This blogpost contains a longer version of Chap. 3 in [my dissertation](). Check it out if you're interested!

Bayesian Optimization (BO) is a tool for black-box optimization. By black-box, we mean functions to which we only have access by querying: expensive simulations of the interaction between a molecule and a folded protein, users interacting with our websites, or the accuracy of a Machine Learning algorithm with a given configuration of hyperparameters.

The gist of BO is to approximate the function we are trying to optimize with a regression model that is uncertainty aware, and to use these uncertainty estimates to determine what our next test point should be. It is usual practice to do BO using Gaussian Processes (GPs), and this blogpost starts with an introduction to GP regression.

This blogpost introduces (Gaussian Process based) Bayesian Optimization, and provides code snippets for the experiments performed. The tools used include GPyTorch and BoTorch as the main engines for Gaussian Processes and BO, and EvoTorch for the evolutionary strategies.

## Running examples: $x\sin(x)$, Easom, and Cross-in-tray

In this blogpost we will use three running examples to explain GPs and BO. The first one is the function $f(x) = x\sin(x)$, which is one-dimensional in its inputs and allows us to visualize how GPs handle uncertainty. The two remaining ones are part of a plethora of test functions that are usually included in black-box optimization benchmarks:[^optimization-benchmarks] `Easom` and `Cross-in-tray`. Their formulas are given by

$$\text{\texttt{Easom}}(\bm{x}) = \cos(x_1)\cos(x_2)\exp\left(-(x_1-\pi)^2 - (x_2 - \pi)^2\right),$$

$$\text{\texttt{Cross-in-tray}}(\bm{x}) = \left|\sin(x_1)\sin(x_2)\exp\left(\left|10 - \frac{\sqrt{x_1^2 + x_2^2}}{\pi}\right|\right)\right|^{0.1}.$$

We take the 2D versions of these functions for visualization, but notice how these can easily be extended to higher dimensions. The `Easom` test function has its optimum at $(\pi, \pi)$, and the `Cross-in-tray` has 4 equally good optima at $(\pm 1.35, \pm 1.35)$, approximately.

<CenteredImage size={"largeSize"} src={"/assets/bo_blogpost/all_three_test_functions.jpg"} alt={"Three test functions: x * sin(x), Easom and Cross-in-tray"} />

[^optimization-benchmarks]: There are plenty more test functions for optimization in [this link](https://www.sfu.ca/~ssurjano/optimization.html).

## An introduction to Gaussian Processes

Gaussian Processes are a **probabilistic regression method**. This means that they approximate a function $f\colon\mathcal{X}\to\mathbb{R}$ while quantifying values like **expectations** and **variances**. More formally,

**Definition:** A function $f$ is a **Gaussian Process** (denoted $f\sim\text{GP}(\mu_0, k)$) if any finite collection of evaluations $\{f(\bm{x}_1), \dots, f(\bm{x}_N)\}$ is normally distributed like

$\begin{bmatrix}
f(\bm{x}_1) \\
\vdots \\
f(\bm{x}_N)
\end{bmatrix} \sim \mathcal{N}\left(\begin{bmatrix}
\mu_0(\bm{x}_1) \\
\vdots \\
\mu_0(\bm{x}_N)
\end{bmatrix}, \begin{bmatrix}
k(\bm{x}_1, \bm{x}_1) & \cdots & k(\bm{x}_1, \bm{x}_N) \\
\vdots & \ddots & \vdots \\
k(\bm{x}_N, \bm{x}_1) & \cdots & k(\bm{x}_N, \bm{x}_N) 
\end{bmatrix}\right)$

The matrix $\bm{K} = [k(\bm{x}_i, \bm{x}_j)]$ is known as the **Gram matrix**, and since we're using it as a covariance matrix, this imposes some restrictions on our choice of **kernel** (or covariance function) $k$: it must be symmetric positive-definite, in the sense that the Gram matrices it spans should be symmetric positive definite. The function $\mu_0$ is called a **prior**, and it allows us to inject expert knowledge in our modeling (if e.g. we know that our function should be something like a line or a parabola, we can set such $\mu_0$). It's pretty common to set $\mu_0 \equiv 0$ (or a constant), and let the data speak for itself.

Assuming $f\sim \text{GP}(\mu, k)$ allows for computing **an entire distribution** over previously unseen points. As an example, let's say you have a dataset $\mathcal{D} = \{(x_1, f(x_1)), \dots, (x_N, f(x_N))\}$ for our test function $f(x)=x\sin(x)$, measured with a little bit of noise: 

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/xsinx_w_noisy_samples.jpg"} alt="Noisy samples from x * sin(x)" />

Given a new point $x_*$ that's not on the dataset, you can consider the joint distribution of $\{f(x_1), \dots, f(x_N), f(x_*)\}$ and compute the conditional distribution of $f(x_*)$ given the dataset:[^the-ugly-details]

[^the-ugly-details]: The math goes as follows...

$$ f(\bm{x}_*) \sim \mathcal{N}(\mu_0(\bm{x}_*) + \bm{k}_*^\top \bm{K}^{-1}(\bm{f} - \bm{\mu_0}), k(\bm{x}_*, \bm{x}_*) - \bm{k}_*^\top \bm{K}^{-1} \bm{k}_*) $$

Consider the distribution around $x_*=0$ in our running example, assuming that the prior $\mu_0$ equals $0$. The closest values we have in the dataset are at $x=-0.7$ and $x=1.1$, which means $x_*=0$ is far away from the training set. This is automatically quantified by the variance in the posterior distribution of $f(x_*)$:

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/xsinx_dist_at_0.jpg"} alt={"A Gaussian distribution with mean and variance predicted by a Gaussian Process, fitted on noisy samples of x * sin(x)"} />

Let's visualize this distribution for all points in the interval [-10, 10], sweeping through them and highlighting the actual value of the function $f(x) = x\sin(x)$.

<CenteredImage size={"largeSize"} src={"/assets/bo_blogpost/xsinx_gp_all_distributions.gif"} alt={"An animation showing several Gaussian distributions predicted by a GP, sweeping through the x axis, of x * sin(x)"} />

Notice how the distribution spikes when close to the training points, and flattens outside the support of the data. This is exactly what we mean by uncertainty quantification.

This process of predicting Gaussian distributions generalizes to collections of previously unseen points $\bm{X}_*$, allowing us consider the mean prediction and the uncertainties around it. The math is essentially the same, and the details can be found in the references.[^the-ugly-details-multidim]

[^the-ugly-details-multidim]: See e.g. [TODO: add ref].

If we consider a fine enough grid of points and consider their posterior distribution, we can essentially "sample a function" out of our Gaussian Process using the same formulas described above. For example, these are 5 different samples of potential approximations of $f(x) = x\sin(x)$ according to the posterior of the GP:[^ugly-detail-RKHS]

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/xsinx_samples.jpg"} alt={"Five samples from a GP fitted on the noisy evaluations of x * sin(x)"} />

Common choices of kernels are the RBF and the MatÃ©rn family:

- $k_{\text{RBF}}(\bm{x},\bm{x}';\; \theta_{\text{out}}, \bm{\theta}_\text{l}) = \theta_{\text{out}}\exp\left(-\frac{1}{2}r(\bm{x}, \bm{x}'; \bm{\theta}_{\text{l}})\right)$

- $k_\nu(\bm{x}, \bm{x}';\; \theta_{\text{out}}, \bm{\theta}_\text{l}) = \theta_{\text{out}}\frac{2^{1 - \nu}}{\Gamma(\nu)}(\sqrt{2\nu}r)^\nu K_\nu(\sqrt{2\nu}r(\bm{x}, \bm{x}'; \bm{\theta}_{\text{l}}))$

[^ugly-detail-RKHS]: This process of sampling functions out of a Gaussian Process can actually be made formal: a kernel $k$ spans a *Reproducing Kernel Hilbert Space* (RKHS), which is a space of functions from which the GP samples [TODO: read a bit more and add here]. Hilbert Spaces are a special kind of vector space, which have both a dot product and a topology that is complete w.r.t. this dot product. If you are curious about the formalities, check [TODO: add reference].

Where
- $r(\bm{x}, \bm{x}'; \bm{\theta}_{\text{l}}) = (\bm{x} - \bm{x}')^\top(\bm{\theta}_{\text{l}})(\bm{x} - \bm{x}')$ is the distance between $\bm{x}$ and $\bm{x}'$ mediated by a diagonal matrix of lengthscales $\bm{\theta}_{\text{l}}$, $\theta_{\text{out}} > 0$ is a positive hyperparameter called the output scale,
- $\Gamma$ is the [Gamma function](TODOADD), $K_\nu$ is the [modified Bessel function](TODOADD) of the second kind, and
- $\nu$ is usually 5/2, but it can be any positive real number. For $\nu = i + 1/2$ (with $i$ a positive integer) the kernel takes a nice closed form.[^details-on-matern]

[^details-on-matern]: For more details, see [Probabilistic Numerics, Sec. 5.5.]().

These kernels have some hyperparameters in them (like the lengthscale matrix $\bm{\theta}_l$ or the output scale $\theta_{\text{out}}$). **Fitting** a Gaussian Process to a given dataset consists of estimating these hyperparameters by maximizing the likelihood of the data.

To summarize,
- By using GPs, we assume that finite collections of evaluations of a function are distributed normally around a certain prior $\mu_0$, with covariances dictated by a kernel $k$.
- This assumption allows us to compute distributions over previously unseen points $\bm{x}_*$.
- Kernels define the space of functions we can use to approximate, and come with certain hyperparameters that we need to fit by maximizing the likelihood.

## Gaussian Processes in practice

### scikit-learn

There are several open source implementations of Gaussian Process Regression, as described above. First, there's `scikit-learn`'s interface, which quickly allows for specifying a kernel. This is the code you would need to create the noisy samples described above, and to fit a Gaussian Process to those samples:

```python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel

import numpy as np

def fit_gaussian_process(
    inputs: np.ndarray, outputs: np.ndarray
) -> GaussianProcessRegressor:
    """
    Fits a Gaussian Process with an RBF kernel
    to the given inputs and outputs.
    """

    # --------- Defining the kernel ---------
    # the "1 *" in front is understood as the
    # output scale, and will be optimized during
    # training. Besides the RBF, we also add a
    # WhiteKernel, which corresponds to adding a
    # constant to the diagonal of the covariance.
    kernel = 1 * RBF() + WhiteKernel()

    # ------ Defining the Gaussian Process -----
    # Besides the kernel, we could also specify the
    # internal optimizer, the number of iterations,
    # etc.
    model = GaussianProcessRegressor(kernel=kernel)
    model.fit(inputs, outputs)

    return model
```

Internally, `scikit-learn` uses `scipy`'s optimizers to maximize the likelihood w.r.t. the kernel parameters. You can check the kernel's optimized parameters by printing `model.kernel_`. These are the hyperparameters for our running example on $f(x) = x\sin(x)$:

```
4.72**2 * RBF(length_scale=1.58) + WhiteKernel(noise_level=0.229)
```

And this is the predicted mean and uncertainty:

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/gp_in_sklearn.jpg"} alt={"A GP fitted on the noisy evaluations of x * sin(x) using an RBF kernel with diagonal noise, using scikit-learn as the backend."} />

A pretty good fit! `scikit-learn`'s implementation is great, but it only deals with the classic **exact** inference, in which we solve for the inverse of the Gram matrix of all the data. This is prohibitively expensive for large datasets, since the complexity of computing this inverse scales cubically with the size of the dataset. More contemporary approaches use approximate versions of this inference.

### GPyTorch

GPyTorch was developed with other forms of Gaussian Processes in mind, plus scalable exact inference. The authors promise exact inference with millions of datapoints, and they achieve this by reducing the computational complexity of inference from $O(n^3)$ to $O(n^2)$. [Check their paper for more details](https://arxiv.org/abs/1809.11165). These other forms of Gaussian Processes include approximations using inducing points, and deep kernel learning (where we use a neural network to project high-dimensional data to a low-dimensional feature space before applying the kernel) (?).

Following the tutorials, this is how you would specify the same Gaussian Process we did for scikit-learn:

```python
import torch

from gpytorch.models import ExactGP
from gpytorch.distributions import MultivariateNormal
from gpytorch.likelihoods import GaussianLikelihood
from gpytorch.means import ZeroMean
from gpytorch.kernels import ScaleKernel, RBFKernel

from gpytorch.mlls import ExactMarginalLogLikelihood


class OurGP(ExactGP):
    def __init__(
        self,
        train_inputs: torch.Tensor,
        train_targets: torch.Tensor,
        likelihood: GaussianLikelihood,
    ):
        super().__init__(train_inputs, train_targets, likelihood)

        # Defining the mean
        # The convention is to call it "mean_module",
        # but I call it mean.
        self.mean = ZeroMean()

        # Defining the kernel
        # The convention is to call it "covar_module",
        # but I usually just call it kernel.
        self.kernel = ScaleKernel(RBFKernel())

    def forward(self, inputs: torch.Tensor) -> MultivariateNormal:
        """
        The forward method is used to compute the
        predictive distribution of the GP.
        """
        mean = self.mean(inputs)
        covar = self.kernel(inputs)

        return MultivariateNormal(mean, covar)


def fit_gaussian_process(
    inputs: torch.Tensor,
    outputs: torch.Tensor,
) -> OurGP:
    """
    Fits a Gaussian Process with an RBF kernel to
    the given inputs and outputs
    """

    # ------- Defining the likelihood ----------
    # The likelihood is the distribution of the outputs
    # given the inputs and the GP.
    likelihood = GaussianLikelihood()

    # ------------ Defining the model ------------
    model = OurGP(inputs, outputs, likelihood)

    # ------------ Training the model ------------
    # The marginal log likelihood is the objective
    # function we want to maximize.
    mll = ExactMarginalLogLikelihood(likelihood, model)
    mll.train()

    # We optimize the marginal likelihood just like
    # optimize any other PyTorch model. The model
    # parameters in this case come from the kernel
    # and likelihood.
    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

    for iteration in range(3000):
        optimizer.zero_grad()
        dist_ = model(inputs)
        loss = -mll(dist_, outputs).mean()
        loss.backward()
        optimizer.step()

        if iteration % 100 == 0:
            print(f"Iteration {iteration + 1}/{3000}: Loss = {loss.item()}")

    # It is IMPORTANT to call model.eval(). This
    # will tell GPyTorch that the forward calls
    # are now inference calls, computing the 
    # posterior distribution with the given
    # kernel hyperparameters.
    model.eval()

    return model
```

Notice that, to train a model using GPyTorch, we need to write significantly more code: specifying that the likelihood is Gaussian,[^other-likelihoods] define the optimizer explicity, and optimize the likelihood manually. This might seem verbose, but it's actually giving us plenty of flexibility! We could e.g. include a neural network in our `forward`, use other likelihoods, or interplay with other PyTorch-based libraries.

[^other-likelihoods]: TODO: discuss e.g. the Categorical likelihood.

You can also see the kernel hyperparameters:

```python
print("lengthscale: ", model.kernel.base_kernel.lengthscale.item())
print("output scale: ", model.kernel.outputscale.item())
print("noise: ", model.likelihood.noise_covar.noise.item())
```

Which outputs something pretty similar to what scikit-learn gave us:

```
lengthscale:  1.5464047193527222
output scale:  20.46845245361328
white noise:  0.23084479570388794
```

And the predictive posterior looks like this:

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/gp_in_gpytorch.jpg"} alt={"A GP fitted on the noisy evaluations of x * sin(x) using an RBF kernel with diagonal noise and outputscale, using GPyTorch as the backend."} />

Pretty similar to scikit-learn! Which is to be expected: the equations are exactly the same, and the kernel and likelihood hyperparameters are close.

### Other tools for Gaussian Process Regression

There are three other tools for building GPs that come to mind:
- [GPy, developed by Neil Lawrence and his team when he was still at Sheffield](https://github.com/SheffieldML/GPy). 
- [GPJax, made by Thomas Pinder](https://github.com/JaxGaussianProcesses/GPJax). Instead of using PyTorch for the backend, it uses Jax. I'd love to try it at some point!
- [GPFlow, made by James Hensman and Alexander G. de G. Matthews](https://github.com/GPflow/GPflow). It uses Tensorflow as the backend.

## A visual introduction to Bayesian Optimization

Bayesian Optimization (B.O.) has three key ingredients:

- A black-box **objective function** $f\colon\mathcal{X}\to\mathbb{R}$, where $\mathcal{X}$ is usually a subset of some $\mathbb{R}^D$, which we are trying to maximize. $f$ is called the objective function.
- A probabilistic surrogate model $\tilde{f}\colon\mathcal{X}\to\mathbb{R}$, whose goal is to approximate the objective function. Since we will use Gaussian Processes, the ingredients we will need are a prior $\mu_0\colon\mathcal{X}\to\mathbb{R}$ and kernel $k$.[^other-surrogate-models]
- An **acquisition function** $\alpha(\bm{x}|\tilde{f})$ which measures the "potential" of new points using the surrogate model. If a given $\bm{x}_*\in\mathcal{X}$ scores high in the acquisition function, then it potentially maximizes the objective function $f$.

[^other-surrogate-models]: We are using Gaussian Processes as a surrogate model here, but other ones could be used! The main requirement is for them to allow us to have good uncertainty estimates of the objective function. [Recent research by AGW has explored this direction]().

The usual loop in a B.O. goes like this:

1. Fit the surrogate model $\tilde{f}$ to the data you have collected so far.
2. Using $\tilde{f}$, optimize the acquisition function $\alpha$ to find the best candidate $\bm{x}$ (i.e. find $\bm{x} = \arg\max \alpha(\cdot|\tilde{f})$).
3. Query the expensive objective function on $\bm{x}$ and repeat from 1 including this new data pair.

We usually stop after finding an optimum that is high enough, or after a fixed number of iterations.[^stopping-criteria]

[^stopping-criteria]: It's not so simple. The stopping criteria of BO are a topic that is currently under plenty of research. See the references.

As we discussed in the introduction, examples of black-box objective functions include the output of expensive simulations in physics and biology, or the accuracy of an ML setup. Now we discuss four examples of acquisition functions.

### Thompson Sampling

The simplest of all acquisition functions is to sample one approximation of $f$ and to optimize it. This can be done quite easily in most probabilistic regression methods, including Gaussian Processes.

Circling back to our guiding example of $x\sin(x)$, let's start from a single random sample and iteratively query the next proposal suggested by Thompson sampling:

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/bo_w_TS.gif"} alt={"A GIF showing how Thompson Sampling works: we fit a Gaussian Process, sample from it, and consider this sample's optimum."} />

### Improvement-based policies

Since we have estimates of the posterior mean $\mu$ and the posterior variance $\sigma^2$ at each step, we have access to a lot more than just a single sample (as in TS). For each new point $\bm{x}_*$, consider its "improvement":

$$ I(\bm{x}_*; \mathcal{D}) = \max(0, f(\bm{x}_*) - f_{\text{best}})$$

where $f_{\text{best}}$ is the maximum value in our trace $\mathcal{D} = \{(\bm{x}_n, f(\bm{x}_n))\}_{n=1}^N$. The improvement measures how much we are gaining at point $f(\bm{x}_*)$, compared to the current best.

since we have a probabilistic approximation of $f$, we can compute quantities like the **probability of improvement** (PI):

$$ \alpha_{\text{PI}}(\bm{x}_*; \mathcal{D}) = \text{Prob}[I(\bm{x}_*;\mathcal{D}) > 0] = \text{Prob}[f(\bm{x}_*) > f_{\text{best}}], $$

In other words, PI measures the probability that a given point will be better than the current maximum. Another quantity we can measure is the **expected improvement** (EI)

$$ \alpha_{\text{EI}}(\bm{x}_*; \mathcal{D}) = \mathbb{E}[I(\bm{x}_*;\mathcal{D})]$$

which measures by how much a given $\bm{x}_*$ may improve on the current best.[^closed-form]

[^closed-form]: These two acquisition functions have closed form when we use vanilla Gaussian Processes. For the technical details, check [TODO: add].

Using the same running example and sample we showed for TS, here's how EI fairs:

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/bo_w_EI.gif"} alt={"A GIF showing how Expected Improvement-based Bayesian Optimization works: we fit a Gaussian Process and use the uncertainty estimates (mean and covariance) to determine the potential improvement of each point in the domain."} />

### Upper-Confidence Bound

Another type of acquisition function is the Upper-Confidence Bound (UCB), which optimistically chooses the points are on the upper bounds of the uncertainty. More explicitly, let $\mu$ and $\sigma$ be the posterior mean and standard deviation on a given point $\bm{x}$ after fitting the GP, then

$$ \alpha_{\text{UCB}}(\bm{x}_*; \mathcal{D}, \beta) = \mu(\bm{x}_*) + \beta\sigma(\bm{x}_*), $$

where $\beta > 0$ is a hyperparameter that states how optimistic we are on the upper bound of the variance. High values of $\beta$ encourage exploration (i.e. exploring regions with higher uncertainty), while lower values of $\beta$ encourage exploitation (staying close to what the posterior mean has learned). To showcase the impact of this hyperparameter, here are two versions of BO on our running example: one with $\beta=1$, and another with $\beta=5$.

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/bo_w_UCB_100.gif"} alt={"A GIF showing how Upper Confidence Bound-based Bayesian Optimization works: we fit a Gaussian Process and use the uncertainty estimates to compute the mean plus a multiple of the standard deviation, which we consider an optimistic choice for where to sample next."} />

Notice that, for $\beta=1$, the optimization gets stuck on the first global optima. This doesn't happen for $\beta=5$:

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/bo_w_UCB_500.gif"} alt={"A GIF showing how Upper Confidence Bound-based Bayesian Optimization works: we fit a Gaussian Process and use the uncertainty estimates to compute the mean plus a multiple of the standard deviation, which we consider an optimistic choice for where to sample next. In this case, the beta is chosen to be 5."} />

## Bayesian Optimization in practice: BoTorch

[TODO: add]

## Bayesian Optimization loops for Easom and Cross-in-Tray

As we discussed above, the way Bayesian Optimization approaches optimization is sequential: given a probabilistic model for the objective function $f$ (which, in our case, is fitted using Gaussian Processes), we find the next candidate by maximizing the acquisition function.

Here are videos showing the sequence of queries in for `Easom` and `Cross-In-Tray`:[^code-link]

[TODO: add Video]

Notice how [TODO: describe the video...]

[^code-link]: [A working version of all the code in this blogpost can be found here](), and [this particular Bayesian Optimization loop can be found here]().

## Bayesian Optimization's drawbacks

- How do we optimize the acquisition function?
- Scaling to high dimensions and big datasets.
- Parallelizing.
- Stopping criteria?

## Conclusion

- Folk knowledge about vanilla BO being only good below 20D
- Comparing with higher dimensional versions of easom and cross-in-tray.
- Let's grab an off-the-shelf TurBO implementation and test it somewhere big (bipedal walker, for example).
- Let's see if CMA-ES/Simple ES does better.

## Cite this blogpost

[TODO: add bibtex].

## References

[More on GPs]

[More on BO]



[More on stopping criteria for BO]


...