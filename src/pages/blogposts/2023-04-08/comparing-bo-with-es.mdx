---
date: "2023-04-08"
title: "Comparing Bayesian Optimization against evolutionary strategies"
summary: "A simple implementation of Bayesian Optimization using GPyTorch and BoTorch, and a comparison with evolutionary strategies like CMA-ES implemented in EvoTorch"
lang: "en"
show: true
imgsrc: "/assets/bo_blogpost/xsinx_gp_all_distributions.gif"
categories:
    - Bayesian Optimization
    - Evolutionary Strategies
    - PyTorch
    - EvoTorch
    - GPyTorch
    - BoTorch
    - Optimization
---

import CenteredImage from './CenteredImage'

# Comparing Bayesian Optimization against Evolutionary Strategies

> This blogpost contains a longer version of a chapter in [my dissertation](). Check it out if you're interested!

Bayesian Optimization (BO) is a tool for black-box optimization. By black-box, we mean functions to which we only have access by querying: expensive simulations of the interaction between a molecule and a folded protein, users interacting with our websites, or the accuracy of a Machine Learning algorithm with a given configuration of hyperparameters.

The gist of BO is to approximate the function we are trying to optimize with a regression model that is uncertainty aware, and to use these uncertainty estimates to determine what our next test point should be. It is usual practice to do BO using Gaussian Processes (GPs), and this blogpost starts with an introduction to GP regression.

BO is not the only alternative for black-box optimization, and in this blogpost we do a short comparison against evolutionary strategies. Folk knowledge says that BO doesn't scale well with the dimensions of the input space (indeed, some people say it doesn't work beyond 20).[^folk]

This blogpost introduces (Gaussian Process based) Bayesian Optimization, and provides code snippets for the experiments performed. The tools used include GPyTorch and BoTorch as the main engines for Gaussian Processes and BO, and EvoTorch for the evolutionary strategies.

Some prerequisites: This blogpost goes pretty fast through what Gaussian Processes are, and how evolutionary strategies work. Links are provided at the beginning of these sections.

[^folk]: TODO Add reference.

## Running examples: $x\sin(x)$, Easom, and Cross-in-tray

In this blogpost we will use three running examples to run the comparison. The first one is the function $f(x) = x\sin(x)$, which is one-dimensional in its inputs and allows us to visualize how GPs handle uncertainty. The two remaining ones are part of a plethora of test functions that are usually included in black-box optimization benchmarks:[^optimization-benchmarks] `Easom` and `Cross-in-tray`. Their formulas are given by

$$\text{\texttt{Easom}}(\bm{x}) = \cos(x_1)\cos(x_2)\exp\left(-(x_1-\pi)^2 - (x_2 - \pi)^2\right),$$

$$\text{\texttt{Cross-in-tray}}(\bm{x}) = \left|\sin(x_1)\sin(x_2)\exp\left(\left|10 - \frac{\sqrt{x_1^2 + x_2^2}}{\pi}\right|\right)\right|^{0.1}.$$

We take the 2D versions of these functions for visualization, but notice how these can easily be extended to higher dimensions. The `Easom` test function has its optimum at $(\pi, \pi)$, and the `Cross-in-tray` has 4 equally good optima at $(\pm 1.35, \pm 1.35)$, approximately.

<CenteredImage size={"largeSize"} src={"/assets/bo_blogpost/all_three_test_functions.jpg"} alt={"Three test functions: x * sin(x), Easom and Cross-in-tray"} />

[^optimization-benchmarks]: There are plenty more test functions for optimization in [this link](https://www.sfu.ca/~ssurjano/optimization.html).

## A short introduction to Gaussian Processes

Gaussian Processes are a **probabilistic regression method**. This means that they approximate a function $f\colon\mathcal{X}\to\mathbb{R}$ while quantifying values like **expectations** and **variances**. More formally,

**Definition:** A function $f$ is a **Gaussian Process** (denoted $f\sim\text{GP}(\mu_0, k)$) if any finite collection of evaluations $\{f(\bm{x}_1), \dots, f(\bm{x}_N)\}$ is normally distributed like

$\begin{bmatrix}
f(\bm{x}_1) \\
\vdots \\
f(\bm{x}_N)
\end{bmatrix} \sim \mathcal{N}\left(\begin{bmatrix}
\mu_0(\bm{x}_1) \\
\vdots \\
\mu_0(\bm{x}_N)
\end{bmatrix}, \begin{bmatrix}
k(\bm{x}_1, \bm{x}_1) & \cdots & k(\bm{x}_1, \bm{x}_N) \\
\vdots & \ddots & \vdots \\
k(\bm{x}_N, \bm{x}_1) & \cdots & k(\bm{x}_N, \bm{x}_N) 
\end{bmatrix}\right)$

The matrix $\bm{K} = [k(\bm{x}_i, \bm{x}_j)]$ is known as the **Gram matrix**, and since we're using it as a covariance matrix, this imposes some restrictions on our choice of **kernel** (or covariance function) $k$: it must be symmetric positive-definite, in the sense that the Gram matrices it spans should be symmetric positive definite. The function $\mu_0$ is called a **prior**, and it allows us to inject expert knowledge in our modeling (if e.g. we know that our function should be something like a line or a parabola, we can set such $\mu_0$). It's pretty common to set $\mu_0 \equiv 0$ (or a constant), and let the data speak for itself.

Assuming $f\sim \text{GP}(\mu, k)$ allows for computing **an entire distribution** over previously unseen points. As an example, let's say you have a dataset $\mathcal{D} = \{(x_1, f(x_1)), \dots, (x_N, f(x_N))\}$ for our test function $f(x)=x\sin(x)$, measured with a little bit of noise: 

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/xsinx_w_noisy_samples.jpg"} alt="Noisy samples from x * sin(x)" />

Given a new point $x_*$ that's not on the dataset, you can consider the joint distribution of $\{f(x_1), \dots, f(x_N), f(x_*)\}$ and compute the conditional distribution of $f(x_*)$ given the dataset:[^the-ugly-details]

[^the-ugly-details]: The math goes as follows...

$$ f(\bm{x}_*) \sim \mathcal{N}(\mu_0(\bm{x}_*) + \bm{k}_*^\top \bm{K}^{-1}(\bm{f} - \bm{\mu_0}), k(\bm{x}_*, \bm{x}_*) - \bm{k}_*^\top \bm{K}^{-1} \bm{k}_*) $$

Consider the distribution around $x_*=0$ in our running example, assuming that the prior $\mu_0$ equals $0$. The closest values we have in the dataset are at $x=-0.7$ and $x=1.1$, which means $x_*=0$ is far away from the training set. This is automatically quantified by the variance in the posterior distribution of $f(x_*)$:

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/xsinx_dist_at_0.jpg"} alt={"A Gaussian distribution with mean and variance predicted by a Gaussian Process, fitted on noisy samples of x * sin(x)"} />

Let's visualize this distribution for all points in the interval [-10, 10], sweeping through them and highlighting the actual value of the function $f(x) = x\sin(x)$.

<CenteredImage size={"largeSize"} src={"/assets/bo_blogpost/xsinx_gp_all_distributions.gif"} alt={"An animation showing several Gaussian distributions predicted by a GP, sweeping through the x axis, of x * sin(x)"} />

Notice how the distribution spikes when close to the training points, and flattens outside the support of the data. This is exactly what we mean by uncertainty quantification.

This process of predicting Gaussian distributions generalizes to collections of previously unseen points $\bm{X}_*$, allowing us consider the mean prediction and the uncertainties around it. The math is essentially the same, and the details can be found in the references.[^the-ugly-details-multidim]

[^the-ugly-details-multidim]: See e.g. [TODO: add ref].

If we consider a fine enough grid of points and consider their posterior distribution, we can essentially "sample a function" out of our Gaussian Process using the same formulas described above. For example, these are 5 different samples of potential approximations of $f(x) = x\sin(x)$ according to the posterior of the GP:[^ugly-detail-RKHS]

<CenteredImage size={"midSize"} src={"/assets/bo_blogpost/xsinx_samples.jpg"} alt={"Five samples from a GP fitted on the noisy evaluations of x * sin(x)"} />

Common choices of kernels are the RBF and the MatÃ©rn family:

- $k_{\text{RBF}}(\bm{x},\bm{x}';\; \theta_{\text{out}}, \bm{\theta}_\text{l}) = \theta_{\text{out}}\exp\left(-\frac{1}{2}r(\bm{x}, \bm{x}'; \bm{\theta}_{\text{l}})\right)$

- $k_\nu(\bm{x}, \bm{x}';\; \theta_{\text{out}}, \bm{\theta}_\text{l}) = \theta_{\text{out}}\frac{2^{1 - \nu}}{\Gamma(\nu)}(\sqrt{2\nu}r)^\nu K_\nu(\sqrt{2\nu}r(\bm{x}, \bm{x}'; \bm{\theta}_{\text{l}}))$

[^ugly-detail-RKHS]: This process of sampling functions out of a Gaussian Process can actually be made formal: a kernel $k$ spans a *Reproducing Kernel Hilbert Space* (RKHS), which is a space of functions from which the GP samples [TODO: read a bit more and add here]. Hilbert Spaces are a special kind of vector space, which have both a dot product and a topology that is complete w.r.t. this dot product. If you are curious about the formalities, check [TODO: add reference].

Where
- $r(\bm{x}, \bm{x}'; \bm{\theta}_{\text{l}}) = (\bm{x} - \bm{x}')^\top(\bm{\theta}_{\text{l}})(\bm{x} - \bm{x}')$ is the distance between $\bm{x}$ and $\bm{x}'$ mediated by a diagonal matrix of lengthscales $\bm{\theta}_{\text{l}}$, $\theta_{\text{out}} > 0$ is a positive hyperparameter,
- $\Gamma$ is the [Gamma function](TODOADD), $K_\nu$ is the [modified Bessel function](TODOADD) of the second kind, and
- $\nu$ is usually 5/2, but it can be any positive real number. For $\nu = i + 1/2$ (with $i$ a positive integer) the kernel takes a nice closed form.[^details-on-matern]

[^details-on-matern]: For more details, see [Probabilistic Numerics, Sec. 5.5.]().

These kernels have some hyperparameters in them (like the lengthscale matrix $\bm{\theta}_l$ or the output scale $\theta_{\text{out}}$). **Fitting** a Gaussian Process to a given dataset consists of estimating these hyperparameters by maximizing the likelihood of the data.

To summarize,
- By using GPs, we assume that finite collections of evaluations of a function are distributed normally around a certain prior $\mu_0$, with covariances dictated by a kernel $k$.
- This assumption allows us to compute distributions over previously unseen points.
- Kernels define the space of functions we can use to approximate, and come with certain hyperparameters that we need to fit by maximizing the likelihood.

## Gaussian Processes in practice

### scikit-learn

There are several open source implementations of Gaussian Process Regression, as described above. First, there's `scikit-learn`'s interface, which quickly allows for specifying a kernel. This is the code you would need to create the noisy samples described above, and to fit a Gaussian Process to those samples:

```python
TODO: ADD
```

Internally, `scikit-learn` uses `scipy`'s optimizers to maximize the likelihood w.r.t. the kernel parameters. You can check the kernel's optimized parameters by printing `model.kernel_`.

`scikit-learn`'s implementation is great, but it only deals with the classic **exact** inference, in which we solve for the inverse of the Gram matrix. This is prohibitively expensive for large datasets, since the complexity of computing this inverse scales cubically with the size of the dataset. More contemporary approaches use approximate versions of this inference.

### GPyTorch

[A section about GPyTorch]

### Other tools

[Jaxpy (?), GPFlow]

## A visual description of Bayesian Optimization

Bayesian Optimization (B.O.) has three key ingredients:

- A black-box **objective function** $f\colon\mathcal{X}\to\mathbb{R}$, where $\mathcal{X}$ is usually a subset of some $\mathbb{R}^D$, which we are trying to maximize. $f$ is called the objective function.
- A probabilistic surrogate model $\tilde{f}\colon\mathcal{X}\to\mathbb{R}$, whose goal is to approximate the objective function. Since we will use Gaussian Processes, the ingredients we will need are a prior $\mu_0\colon\mathcal{X}\to\mathbb{R}$ and kernel $k$.[^other-surrogate-models]
- An **acquisition function** $\alpha(\bm{x}|\tilde{f})$ which measures the "potential" of new points using the surrogate model. If a given $\bm{x}_*\in\mathcal{X}$ scores high in the acquisition function, then it potentially maximizes the objective function $f$.

[^other-surrogate-models]: We are using Gaussian Processes as a surrogate model here, but other ones could be used! The main requirement is for them to allow us to have good uncertainty estimates of the objective function. [Recent research by AGW has explored this direction]().

The usual loop in a B.O. goes like this:

1. Fit the surrogate model $\tilde{f}$ to the data you have collected so far.
2. Using $\tilde{f}$, query the acquisition function $\alpha$ to find the best candidate $\bm{x}$ (i.e. find $\bm{x} = \arg\max \alpha(\cdot|\tilde{f})$).
3. Query the expensive objective function on $\bm{x}$ and repeat from 1 including this new data pair.

We usually stop after finding an optimum that is high enough, or after a fixed number of iterations.[^stopping-criteria]

[^stopping-criteria]: It's not so simple. The stopping criteria of BO are a topic that is currently under plenty of research. See the references.

As we discussed in the introduction, examples of black-box objective functions include the output of expensive simulations in physics and biology, or the accuracy of an ML setup. Now we discuss four examples of acquisition functions.

### Thompson Sampling

The simplest of all acquisition functions is to sample one approximation of $f$ and to optimize it. This can be done quite easily in most probabilistic regression methods, including Gaussian Processes.

Circling back to our guiding example of $x\sin(x)$, let's start from a single random sample and iteratively query the next proposal suggested by Thompson sampling:

[TODO: build this animation]

### Improvement-based policies

Since we have estimates of the posterior mean $\mu$ and the posterior variance $\sigma^2$ at each step, we have access to a lot more than just a single sample (as in TS). For each new point $\bm{x}_*$, consider its "improvement":

$$ I(\bm{x}_*; \mathcal{D}) = \max(0, f(\bm{x}_*) - f_{\text{best}})$$

where $f_{\text{best}}$ is the maximum value in our trace $\mathcal{D} = \{(\bm{x}_n, f(\bm{x}_n))\}_{n=1}^N$. The improvement measures how much we are gaining at point $f(\bm{x}_*)$, compared to the current best.

since we have a probabilistic approximation of $f$, we can compute quantities like the **probability of improvement** (PI):

$$ \alpha_{\text{PI}}(\bm{x}_*; \mathcal{D}) = \text{Prob}[I(\bm{x}_*;\mathcal{D}) > 0] = \text{Prob}[f(\bm{x}_*) > f_{\text{best}}], $$

In other words, PI measures the probability that a given point will be better than the current maximum. Another quantity we can measure is the **expected improvement** (EI)

$$ \alpha_{\text{EI}}(\bm{x}_*; \mathcal{D}) = \mathbb{E}[I(\bm{x}_*;\mathcal{D})] = ..., $$

which measures by how much a given $\bm{x}_*$ may improve on the current best.

Using the same running example and sample we showed for TS, here are the improvement-based policies visually: [TODO: add and explain].

### Upper-Confidence Bound

Another type of acquisition function is the Upper-Confidence Bound (UCB), which optimistically chooses the points are on the upper bounds of the uncertainty. More explicitly, let $\mu$ and $\sigma$ be the posterior mean and standard deviation on a given point $\bm{x}$ after fitting the GP, then

$$ \alpha_{\text{UCB}}(\bm{x}_*; \mathcal{D}) = \mu(\bm{x}_*) + \kappa\sigma(\bm{x}_*), $$

where $\kappa > 0$ is a hyperparameter that states how optimistic we are on the upper bound of the variance. High values of $\kappa$ encourage exploration (i.e. exploring regions with higher uncertainty), while lower values of $\kappa$ encourage exploitation (staying close to what the posterior mean has learned). Here are some examples of the UCB acquisition function:

[TODO: add]

## Bayesian Optimization in practice: BoTorch

[TODO: add]

## What people say and do

[A discussion on the folk choices of acq. function]

## About high dimensional BO

[High dimensional BO doesn't really work that well, and we have to rely on other alternatives]

## Can we parallelize?

[A discussion on parallelizability]

## Optimizing the acquisition might be hard

[Optimizing it might be hard. Differential approximations. CMA-ES inside BO]

## Bayesian Optimization loops for our examples

As we discussed above, the way Bayesian Optimization approaches optimization is sequential: given a probabilistic model for the objective function $f$ (which, in our case, is fitted using Gaussian Processes), we find the next candidate by maximizing the acquisition function.

Here are videos showing the sequence of queries in for `Easom` and `Cross-In-Tray`:[^code-link]

[TODO: add Video]

Notice how [TODO: describe the video...]

[^code-link]: [A working version of all the code in this blogpost can be found here](), and [this particular Bayesian Optimization loop can be found here]().

### BO's drawbacks

- How do we optimize the acquisition function?
- Scaling to high dimensions and big datasets.
- Parallelizing.
- Stopping criteria?

## A couple of evolutionary strategies

Instead of building a model of the objective function $f$, evolutionary strategies (ES) maintain a mean $m$ and covariance matrix $\Sigma$ which parametrize a Gaussian distribution around the current best-performing elements of the search space. At each step, an ES algorithm samples a population centered at $m$ with covariance $\Sigma$, and updates them according to the new maxima found (if any).

This section briefly introduces a couple ES. [A great visual introduction (which inspired this blogpost) is available here.]()

### Simple ES

The simplest version of ES is to consider a fixed covariance $\Sigma = s\bm{I}_{D}$ where $\bm{I}_D$ is the identity matrix of size $D$, the size of the search space $\mathcal{X}$, and $s>0$ is a scale hyperparameter that governs the size of the covariance. The update rule only considers the mean, and computes it either greedily as the single best performing element, or as the average of e.g. 25% of the best performing members of the population.

Here's a simple ES in practice in Easom and in Cross-in-tray. Depending on the size of the scale $s$, a simple ES algorithm might get stuck in local optima.

[TODO: add video]


### CMA-ES

A smarter strategy is to adapt the covariance matrix, shaping it by only the best fourth of the population. The rule for adapting the distribution becomes

[TODO: add rule for adaptation]

Similarly, we can run CMA-ES on Easom and Cross-in-tray:

[...]

### ES' drawbacks

- Sometimes $D$ is so large that storing a $D^2$ matrix (the covariance) is prohibitive.
- It's highly sample inefficient.
- That's it, I think.

## A comparison: sample efficiency

In these examples we have shown a single trace on our test functions. What happens if we run several? Will these optimization algorithms always converge? And if so, how many queries to the objective function did they require?

Notice that, in general, evolutionary strategies query the objective function in a large number of points for each generation (i.e. the population size). On the other hand, Bayesian Optimization queries one point per iteration. We can expect Bayesian Optimization to converge with less queries than evolutionary strategies.

In this final example, we run the evolutionary strategies and Bayesian Optimization on both test function 100 times, stopping after finding an optima that is at least 95% the global optima. The results go as follows:

## Conclusion

- Folk knowledge about vanilla BO being only good below 20D
- Comparing with higher dimensional versions of easom and cross-in-tray.
- Let's grab an off-the-shelf TurBO implementation and test it somewhere big (bipedal walker, for example).
- Let's see if CMA-ES/Simple ES does better.

## Cite this blogpost

[TODO: add bibtex].

## References

[More on GPs]

[More on BO]



[More on stopping criteria for BO]


...